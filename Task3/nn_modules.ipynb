{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_modules.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andre6o6/mlcourse-2019/blob/master/Task3/nn_modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRJf5WYFKAAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpcA5dYgKtTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Forward:\n",
        "        \n",
        "                    input -> [module] -> output\n",
        "\n",
        "    Backward:\n",
        "\n",
        "        grad(w.r.t.)Input <- [module] <- grad(w.r.t.)Output\n",
        "                                      ^- input\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "        \n",
        "        This includes \n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "    \n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field. \n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "            \n",
        "        # self.output = input \n",
        "        # return self.output\n",
        "        \n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input. \n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "        \n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "        \n",
        "        # self.gradInput = gradOutput \n",
        "        # return self.gradInput\n",
        "        \n",
        "        pass   \n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def zeroGradParameters(self): \n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "        \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Module\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrOXro-oLL2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially. \n",
        "         \n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "   \n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "        \n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})   \n",
        "        \"\"\"\n",
        "        x = input\n",
        "        for i in range(len(self.modules)):\n",
        "            x = self.modules[i].forward(x)\n",
        "        self.output = x\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "            \n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)   \n",
        "            gradInput = module[0].backward(input, g_1)        \n",
        "        \"\"\"\n",
        "        g = gradOutput\n",
        "        for i in reversed(range(len(self.modules))):\n",
        "            y_prev = input if i==0 else self.modules[i-1].output\n",
        "            g = self.modules[i].backward(y_prev, g)\n",
        "        self.gradInput = g\n",
        "        return self.gradInput\n",
        "      \n",
        "\n",
        "    def zeroGradParameters(self): \n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "    \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "    \n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atnGzYTdQxhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation \n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
        "    \n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "       \n",
        "        # Xavier initialization (but with uniform distribution(?))\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_in, n_out))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "        \n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "        \n",
        "    def updateOutput(self, input):        \n",
        "        self.output = np.matmul(input, self.W) + self.b\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.matmul(gradOutput, self.W.T)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradW = np.matmul(input.T, gradOutput)\n",
        "        self.gradb = np.sum(gradOutput, axis=0)\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[0],s[1])\n",
        "        return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svWbkwF5Yrhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exps = np.exp(shiftx)\n",
        "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        b,n = gradOutput.shape\n",
        "        self.gradInput = np.zeros_like(self.output)\n",
        "        for i in range(b):\n",
        "            jacobian = self.output[i].reshape(-1,1) * (np.eye(n) - self.output[i])\n",
        "            self.gradInput[i] = np.matmul(gradOutput[i], jacobian)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PbbkI2obcBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = 0 \n",
        "        self.moving_variance = 0\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        if (self.training):\n",
        "            batch_mean = np.mean(input, axis=0)\n",
        "            batch_variance = np.var(input, axis=0)\n",
        "\n",
        "            self.moving_mean = self.moving_mean * self.alpha + batch_mean * (1 - self.alpha)\n",
        "            self.moving_variance = self.moving_variance * self.alpha + batch_variance * (1 - self.alpha)\n",
        "\n",
        "            self.output = (input - batch_mean) / np.sqrt(batch_variance + self.EPS)\n",
        "        else:\n",
        "            self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        mu = np.mean(input, axis=0)\n",
        "        var = np.var(input, axis=0)\n",
        "        \n",
        "        B = input.shape[0]        #TODO check dims\n",
        "        self.gradInput = 1 / np.sqrt(var + self.EPS) * (gradOutput - 1/B * np.sum(gradOutput, axis=0) \\\n",
        "                                                        - 1/B * (input - mu) / (var + self.EPS) * np.sum(gradOutput * (input - mu), axis=0))\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzgotnPvE6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        \n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "        \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yi-lJJfcUje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "        \n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        return  self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        return self.gradInput\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1nkBaS_cgoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c12q9aKsctcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function \n",
        "            associated to the criterion and return the result.\n",
        "            \n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result. \n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "    \n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput   \n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laolM8U6czBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target):   \n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output \n",
        " \n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx6uquDYkiMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        self.output = shiftx - np.log(np.sum(np.exp(shiftx), axis=1, keepdims=True))\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exps = np.exp(shiftx)\n",
        "        softmax = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        self.gradInput = gradOutput - softmax * np.sum(gradOutput, axis=1, keepdims=True)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvNZ66NwkRkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(NLLCriterion, self)\n",
        "        super(NLLCriterion, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target):\n",
        "        B = input.shape[0]\n",
        "        loss = - input[range(B), target]\n",
        "        self.output = np.sum(loss) / B\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        B = input.shape[0]\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        self.gradInput[range(B), target] = - 1/B\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJqzdvXc2Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossEntropyLoss(Criterion):\n",
        "    \"\"\"\n",
        "    Combines LogSoftmax and NLLLoss in one single class.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CrossEntropyLoss, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target):   \n",
        "        \"\"\"\n",
        "        Expects labels as targets, not onehot vectors\n",
        "        \"\"\"\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        loss = - shiftx[range(shiftx.shape[0]), target] + np.log(np.sum(np.exp(shiftx), axis=1))\n",
        "        self.output = np.sum(loss) / input.shape[0]\n",
        "        return self.output\n",
        " \n",
        "    def updateGradInput(self, input, target):\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exps = np.exp(shiftx)\n",
        "        softmax = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        softmax[range(input.shape[0]), target] -= 1\n",
        "\n",
        "        self.gradInput = softmax / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"CrossEntropyLoss\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}