{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task3_AIO with notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andre6o6/mlcourse-2019/blob/master/task3_AIO_with_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_V3g_cEqqvI",
        "colab_type": "text"
      },
      "source": [
        "Homework: https://drive.google.com/file/d/0B8Duph3WCZx2Q0FNSm5DczF2T0Rzck05TTUtYW84LUxfZTg4/view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoGqEb3mbPA0",
        "colab_type": "text"
      },
      "source": [
        "Helpfull info:\n",
        "\n",
        "* http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
        "* http://cs231n.github.io/neural-networks-case-study/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRJf5WYFKAAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpcA5dYgKtTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Forward:\n",
        "        \n",
        "                    input -> [module] -> output\n",
        "\n",
        "    Backward:\n",
        "\n",
        "        grad(w.r.t.)Input <- [module] <- grad(w.r.t.)Output\n",
        "                                      ^- input\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "        \n",
        "        This includes \n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "    \n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field. \n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "            \n",
        "        # self.output = input \n",
        "        # return self.output\n",
        "        \n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input. \n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "        \n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "        \n",
        "        # self.gradInput = gradOutput \n",
        "        # return self.gradInput\n",
        "        \n",
        "        pass   \n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def zeroGradParameters(self): \n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "        \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Module\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrOXro-oLL2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially. \n",
        "         \n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "   \n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "        \n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})   \n",
        "        \"\"\"\n",
        "        x = input\n",
        "        for i in range(len(self.modules)):\n",
        "            x = self.modules[i].forward(x)\n",
        "        self.output = x\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "            \n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)   \n",
        "            gradInput = module[0].backward(input, g_1)        \n",
        "        \"\"\"\n",
        "        g = gradOutput\n",
        "        for i in reversed(range(len(self.modules))):\n",
        "            y_prev = input if i==0 else self.modules[i-1].output\n",
        "            g = self.modules[i].backward(y_prev, g)\n",
        "        self.gradInput = g\n",
        "        return self.gradInput\n",
        "      \n",
        "\n",
        "    def zeroGradParameters(self): \n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "    \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "    \n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU_Ip0H8cMGI",
        "colab_type": "text"
      },
      "source": [
        "See: http://cs231n.stanford.edu/handouts/linear-backprop.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atnGzYTdQxhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation \n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
        "    \n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "       \n",
        "        # Xavier initialization (but with uniform distribution(?))\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_in, n_out))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "        \n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "        \n",
        "    def updateOutput(self, input):        \n",
        "        self.output = np.matmul(input, self.W) + self.b\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.matmul(gradOutput, self.W.T)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradW = np.matmul(input.T, gradOutput)\n",
        "        self.gradb = np.sum(gradOutput, axis=0)\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[0],s[1])\n",
        "        return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcKsmfrUfFGI",
        "colab_type": "text"
      },
      "source": [
        "See: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svWbkwF5Yrhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exps = np.exp(shiftx)\n",
        "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        b,n = gradOutput.shape\n",
        "        self.gradInput = np.zeros_like(self.output)\n",
        "        for i in range(b):\n",
        "            jacobian = self.output[i].reshape(-1,1) * (np.eye(n) - self.output[i])\n",
        "            self.gradInput[i] = np.matmul(gradOutput[i], jacobian)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idtQhKj1PaJG",
        "colab_type": "text"
      },
      "source": [
        "See: https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PbbkI2obcBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = 0 \n",
        "        self.moving_variance = 0\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        if (self.training):\n",
        "            batch_mean = np.mean(input, axis=0)\n",
        "            batch_variance = np.var(input, axis=0)\n",
        "\n",
        "            self.moving_mean = self.moving_mean * self.alpha + batch_mean * (1 - self.alpha)\n",
        "            self.moving_variance = self.moving_variance * self.alpha + batch_variance * (1 - self.alpha)\n",
        "\n",
        "            self.output = (input - batch_mean) / np.sqrt(batch_variance + self.EPS)\n",
        "        else:\n",
        "            self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        mu = np.mean(input, axis=0)\n",
        "        var = np.var(input, axis=0)\n",
        "        \n",
        "        B = input.shape[0]        #TODO check dims\n",
        "        self.gradInput = 1 / np.sqrt(var + self.EPS) * (gradOutput - 1/B * np.sum(gradOutput, axis=0) \\\n",
        "                                                        - 1/B * (input - mu) / (var + self.EPS) * np.sum(gradOutput * (input - mu), axis=0))\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzgotnPvE6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        \n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "        \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yi-lJJfcUje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "        \n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        return  self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        return self.gradInput\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1nkBaS_cgoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c12q9aKsctcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function \n",
        "            associated to the criterion and return the result.\n",
        "            \n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result. \n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "    \n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput   \n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laolM8U6czBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target):   \n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output \n",
        " \n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx6uquDYkiMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        self.output = shiftx - np.log(np.sum(np.exp(shiftx), axis=1, keepdims=True))\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exps = np.exp(shiftx)\n",
        "        softmax = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        self.gradInput = gradOutput - softmax * np.sum(gradOutput, axis=1, keepdims=True)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvNZ66NwkRkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(NLLCriterion, self)\n",
        "        super(NLLCriterion, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target):\n",
        "        B = input.shape[0]\n",
        "        loss = - input[range(B), target]\n",
        "        self.output = np.sum(loss) / B\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        B = input.shape[0]\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        self.gradInput[range(B), target] = - 1/B\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJqzdvXc2Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossEntropyLoss(Criterion):\n",
        "    \"\"\"\n",
        "    Combines LogSoftmax and NLLLoss in one single class.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CrossEntropyLoss, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target):   \n",
        "        \"\"\"\n",
        "        Expects labels as targets, not onehot vectors\n",
        "        \"\"\"\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        loss = - shiftx[range(shiftx.shape[0]), target] + np.log(np.sum(np.exp(shiftx), axis=1))\n",
        "        self.output = np.sum(loss) / input.shape[0]\n",
        "        return self.output\n",
        " \n",
        "    def updateGradInput(self, input, target):\n",
        "        shiftx = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exps = np.exp(shiftx)\n",
        "        softmax = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        softmax[range(input.shape[0]), target] -= 1\n",
        "\n",
        "        self.gradInput = softmax / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"CrossEntropyLoss\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whQ08vfXbC9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TESTING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvZJLwSJMlus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy\n",
        "import unittest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB4bPx3AMbcj",
        "colab_type": "code",
        "outputId": "af3fc4d4-4528-4b95-f42d-2e7d6782421c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "class TestLayers(unittest.TestCase):\n",
        "    def test_Linear(self):\n",
        "        np.random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        batch_size, n_in, n_out = 2, 3, 4\n",
        "        for _ in range(100):\n",
        "            # layers initialization\n",
        "            torch_layer = torch.nn.Linear(n_in, n_out)\n",
        "            custom_layer = Linear(n_in, n_out)\n",
        "            custom_layer.W = torch_layer.weight.data.numpy().T\n",
        "            custom_layer.b = torch_layer.bias.data.numpy()\n",
        "\n",
        "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
        "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n",
        "\n",
        "            # 1. check layer output\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
        "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
        "            torch_layer_output_var = torch_layer(layer_input_var)\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
        "        \n",
        "            # 2. check layer input grad\n",
        "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
        "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
        "            torch_layer_grad_var = layer_input_var.grad\n",
        "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
        "\n",
        "            # 3. check layer parameters grad\n",
        "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
        "            weight_grad = custom_layer.gradW\n",
        "            bias_grad = custom_layer.gradb\n",
        "            torch_weight_grad = torch_layer.weight.grad.data.numpy().T\n",
        "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
        "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
        "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
        "\n",
        "    def test_SoftMax(self):\n",
        "        np.random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        batch_size, n_in = 2, 4\n",
        "        for _ in range(100):\n",
        "            # layers initialization\n",
        "            torch_layer = torch.nn.Softmax(dim=1)\n",
        "            custom_layer = SoftMax()\n",
        "\n",
        "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
        "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
        "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
        "            next_layer_grad = next_layer_grad.clip(1e-5,1.)\n",
        "            next_layer_grad = 1. / next_layer_grad\n",
        "\n",
        "            # 1. check layer output\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
        "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
        "            torch_layer_output_var = torch_layer(layer_input_var)\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n",
        "\n",
        "            # 2. check layer input grad\n",
        "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
        "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
        "            torch_layer_grad_var = layer_input_var.grad\n",
        "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
        "\n",
        "    def test_LogSoftMax(self):\n",
        "        np.random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        batch_size, n_in = 2, 4\n",
        "        for _ in range(100):\n",
        "            # layers initialization\n",
        "            torch_layer = torch.nn.LogSoftmax(dim=1)\n",
        "            custom_layer = LogSoftMax()\n",
        "\n",
        "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
        "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
        "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
        "\n",
        "            # 1. check layer output\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
        "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
        "            torch_layer_output_var = torch_layer(layer_input_var)\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
        "\n",
        "            # 2. check layer input grad\n",
        "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
        "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
        "            torch_layer_grad_var = layer_input_var.grad\n",
        "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
        "\n",
        "    def test_BatchNormalization(self):\n",
        "        np.random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        batch_size, n_in = 32, 16\n",
        "        for _ in range(100):\n",
        "            # layers initialization\n",
        "            slope = np.random.uniform(0.01, 0.05)\n",
        "            alpha = 0.9\n",
        "            custom_layer = BatchNormalization(alpha)\n",
        "            custom_layer.train()\n",
        "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n",
        "            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
        "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
        "\n",
        "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
        "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
        "\n",
        "            # 1. check layer output\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
        "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
        "            torch_layer_output_var = torch_layer(layer_input_var)\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
        "\n",
        "            # 2. check layer input grad\n",
        "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
        "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
        "            torch_layer_grad_var = layer_input_var.grad\n",
        "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
        "\n",
        "            # 3. check moving mean\n",
        "            self.assertTrue(np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy()))\n",
        "            # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
        "            # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
        "            #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
        "\n",
        "            # 4. check evaluation mode\n",
        "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
        "            custom_layer.evaluate()\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
        "            torch_layer.eval()\n",
        "            torch_layer_output_var = torch_layer(layer_input_var)\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
        "\n",
        "    def test_Sequential(self):\n",
        "        np.random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        batch_size, n_in = 2, 4\n",
        "        for _ in range(100):\n",
        "            # layers initialization\n",
        "            alpha = 0.9\n",
        "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n",
        "            torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n",
        "            custom_layer = Sequential()\n",
        "            bn_layer = BatchNormalization(alpha)\n",
        "            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
        "            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
        "            custom_layer.add(bn_layer)\n",
        "            scaling_layer = ChannelwiseScaling(n_in)\n",
        "            scaling_layer.gamma = torch_layer.weight.data.numpy()\n",
        "            scaling_layer.beta = torch_layer.bias.data.numpy()\n",
        "            custom_layer.add(scaling_layer)\n",
        "            custom_layer.train()\n",
        "\n",
        "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
        "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
        "\n",
        "            # 1. check layer output\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
        "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
        "            torch_layer_output_var = torch_layer(layer_input_var)\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
        "\n",
        "            # 2. check layer input grad\n",
        "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
        "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
        "            torch_layer_grad_var = layer_input_var.grad\n",
        "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
        "\n",
        "            # 3. check layer parameters grad\n",
        "            weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
        "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
        "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
        "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
        "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
        "\n",
        "    def test_NLLCriterion(self):\n",
        "        np.random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        batch_size, n_in = 2, 4\n",
        "        for _ in range(100):\n",
        "            # layers initialization\n",
        "            torch_layer = torch.nn.NLLLoss()\n",
        "            custom_layer = NLLCriterion()\n",
        "\n",
        "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
        "            layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n",
        "            target_labels = np.random.choice(n_in, batch_size)\n",
        "            #target = np.zeros((batch_size, n_in), np.float32)\n",
        "            #target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
        "\n",
        "            # 1. check layer output\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input, target_labels)\n",
        "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
        "            torch_layer_output_var = torch_layer(layer_input_var, \n",
        "                                                 Variable(torch.from_numpy(target_labels), requires_grad=False))\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
        "\n",
        "            # 2. check layer input grad\n",
        "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target_labels)\n",
        "            torch_layer_output_var.backward()\n",
        "            torch_layer_grad_var = layer_input_var.grad\n",
        "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
        "\n",
        "    def test_CrossEntropyLoss(self):\n",
        "        np.random.seed(42)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        batch_size, n_in = 2, 4\n",
        "        for _ in range(100):\n",
        "            # layers initialization\n",
        "            torch_layer = torch.nn.CrossEntropyLoss()\n",
        "            custom_layer = CrossEntropyLoss()\n",
        "\n",
        "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
        "            layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n",
        "            target_labels = np.random.choice(n_in, batch_size)\n",
        "            #target = np.zeros((batch_size, n_in), np.float32)\n",
        "            #target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
        "\n",
        "            # 1. check layer output\n",
        "            custom_layer_output = custom_layer.updateOutput(layer_input, target_labels)\n",
        "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
        "            torch_layer_output_var = torch_layer(layer_input_var, \n",
        "                                                 Variable(torch.from_numpy(target_labels), requires_grad=False))\n",
        "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
        "\n",
        "            # 2. check layer input grad\n",
        "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target_labels)\n",
        "            torch_layer_output_var.backward()\n",
        "            torch_layer_grad_var = layer_input_var.grad\n",
        "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
        "\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_BatchNormalization (__main__.TestLayers) ... ok\n",
            "test_ClassNLLCriterion (__main__.TestLayers) ... ok\n",
            "test_CrossEntropyLoss (__main__.TestLayers) ... ok\n",
            "test_Linear (__main__.TestLayers) ... ok\n",
            "test_LogSoftMax (__main__.TestLayers) ... ok\n",
            "test_Sequential (__main__.TestLayers) ... ok\n",
            "test_SoftMax (__main__.TestLayers) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 7 tests in 0.607s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=7 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDVgrnKU9ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRAINING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGB5goyfU9rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
        "#!wget yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
        "#!wget yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
        "#!wget yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Wnfcm9YWdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!gzip -d train-images-idx3-ubyte.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZWcyxFnaDk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "3670f40c-2a27-444f-ea04-42fae15517b7"
      },
      "source": [
        "!wget https://www.python-course.eu/data/mnist/mnist_train.csv\n",
        "!wget https://www.python-course.eu/data/mnist/mnist_test.csv"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-26 09:41:10--  https://www.python-course.eu/data/mnist/mnist_train.csv\n",
            "Resolving www.python-course.eu (www.python-course.eu)... 138.201.17.115, 2a01:4f8:171:286f::4\n",
            "Connecting to www.python-course.eu (www.python-course.eu)|138.201.17.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 109575994 (104M) [text/csv]\n",
            "Saving to: ‘mnist_train.csv’\n",
            "\n",
            "mnist_train.csv     100%[===================>] 104.50M  27.1MB/s    in 4.4s    \n",
            "\n",
            "2019-11-26 09:41:20 (23.5 MB/s) - ‘mnist_train.csv’ saved [109575994/109575994]\n",
            "\n",
            "--2019-11-26 09:41:21--  https://www.python-course.eu/data/mnist/mnist_test.csv\n",
            "Resolving www.python-course.eu (www.python-course.eu)... 138.201.17.115, 2a01:4f8:171:286f::4\n",
            "Connecting to www.python-course.eu (www.python-course.eu)|138.201.17.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18289443 (17M) [text/csv]\n",
            "Saving to: ‘mnist_test.csv’\n",
            "\n",
            "mnist_test.csv      100%[===================>]  17.44M  13.7MB/s    in 1.3s    \n",
            "\n",
            "2019-11-26 09:41:23 (13.7 MB/s) - ‘mnist_test.csv’ saved [18289443/18289443]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6oJyuVsa38P",
        "colab_type": "code",
        "outputId": "234c8344-29a8-4816-e2df-3a2573d37c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "image_size = 28\n",
        "image_pixels = image_size * image_size\n",
        "data_path = \"\"\n",
        "train_data = np.loadtxt(data_path + \"mnist_train.csv\", \n",
        "                        delimiter=\",\")\n",
        "test_data = np.loadtxt(data_path + \"mnist_test.csv\", \n",
        "                        delimiter=\",\")\n",
        "train_data[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [4., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [3., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [4., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NHauKeVbfTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = 2 * np.asfarray(train_data[:, 1:])/255 - 1\n",
        "y_train = np.asarray(train_data[:, :1], dtype=int)\n",
        "\n",
        "X_test = 2 * np.asfarray(test_data[:, 1:])/255 - 1\n",
        "y_test = np.asarray(test_data[:, :1], dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8My1eTobL3A",
        "colab_type": "code",
        "outputId": "02bf1f2b-9bd2-4882-ee4b-da9495fc1dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(20,15))\n",
        "axes = axes.flatten()\n",
        "for img, ax in zip(X_train[:5].reshape((-1,28,28)), axes):\n",
        "    ax.imshow(img, cmap=\"Greys\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAAEeCAYAAAAHC3ASAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyAmOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x1080 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsI6ldbGctfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(X,y,ratio=0.1):\n",
        "    n_samples = X.shape[0]\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    n = int(n_samples*ratio)\n",
        "    test_indices = indices[:n]\n",
        "    train_indices = indices[n:]\n",
        "\n",
        "    return X[train_indices], y[train_indices], X[test_indices], y[test_indices]\n",
        "\n",
        "X_train, y_train, X_val, y_val = train_test_split(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYD3Xl2wYdaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch generator\n",
        "def get_batches(dataset, batch_size):\n",
        "    X, Y = dataset\n",
        "    n_samples = X.shape[0]\n",
        "        \n",
        "    # Shuffle at the start of epoch\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "    \n",
        "        yield X[batch_idx], Y[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aUpoDryXKXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd_momentum(x, dx, config, state):\n",
        "    \"\"\"\n",
        "        This is a very ugly implementation of sgd with momentum \n",
        "        just to show an example how to store old grad in state.\n",
        "        \n",
        "        config:\n",
        "            - momentum\n",
        "            - learning_rate\n",
        "        state:\n",
        "            - old_grad\n",
        "    \"\"\"\n",
        "    \n",
        "    # x and dx have complex structure, old dx will be stored in a simpler one\n",
        "    state.setdefault('old_grad', {})\n",
        "    \n",
        "    i = 0 \n",
        "    for cur_layer_x, cur_layer_dx in zip(x,dx): \n",
        "        for cur_x, cur_dx in zip(cur_layer_x,cur_layer_dx):\n",
        "            \n",
        "            cur_old_grad = state['old_grad'].setdefault(i, np.zeros_like(cur_dx))\n",
        "            np.add(config['momentum'] * cur_old_grad, config['learning_rate'] * cur_dx, out = cur_old_grad)\n",
        "            \n",
        "            cur_x -= cur_old_grad\n",
        "            i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LZKwfaHXvQ-",
        "colab_type": "code",
        "outputId": "1418b464-9448-4dc5-951e-39c44455c7ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "net = Sequential()\n",
        "net.add(Linear(784, 128))\n",
        "net.add(BatchNormalization())\n",
        "net.add(ChannelwiseScaling(128))\n",
        "net.add(ReLU())\n",
        "net.add(Linear(128, 128))\n",
        "net.add(BatchNormalization())\n",
        "net.add(ChannelwiseScaling(128))\n",
        "net.add(ReLU())\n",
        "net.add(Linear(128, 10))\n",
        "net.add(LogSoftMax())\n",
        "\n",
        "criterion = NLLCriterion()\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear 784 -> 128\n",
            "BatchNormalization\n",
            "ChannelwiseScaling\n",
            "ReLU\n",
            "Linear 128 -> 128\n",
            "BatchNormalization\n",
            "ChannelwiseScaling\n",
            "ReLU\n",
            "Linear 128 -> 10\n",
            "LogSoftMax\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o7NKthZYb62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer params\n",
        "optimizer_config = {'learning_rate' : 0.5, 'momentum': 0.9}\n",
        "optimizer_state = {}\n",
        "\n",
        "# Looping params\n",
        "n_epoch = 6\n",
        "batch_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2om8HZ1TLQ5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_iters = X_train.shape[0] // batch_size\n",
        "if X_train.shape[0] % batch_size > 0:\n",
        "    n_iters += 1\n",
        "\n",
        "n_val_iters = X_val.shape[0] // batch_size\n",
        "if X_val.shape[0] % batch_size > 0:\n",
        "    n_val_iters += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spLBPcXpc4ss",
        "colab_type": "code",
        "outputId": "12a9bd8d-83cf-40b9-95f0-6358940eb946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "from IPython import display\n",
        "\n",
        "history = []\n",
        "val_losses = []\n",
        "for i in range(n_epoch):\n",
        "    net.train()\n",
        "    \n",
        "    # LR schedule\n",
        "    if i>=2:\n",
        "        optimizer_config['learning_rate'] = 0.05\n",
        "\n",
        "    train_loss = 0\n",
        "    for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
        "        y_batch = y_batch.reshape(-1)\n",
        "        \n",
        "        net.zeroGradParameters()\n",
        "        \n",
        "        # Forward\n",
        "        predictions = net.forward(x_batch)\n",
        "        loss = criterion.forward(predictions, y_batch)\n",
        "    \n",
        "        # Backward\n",
        "        dp = criterion.backward(predictions, y_batch)\n",
        "        net.backward(x_batch, dp)\n",
        "        \n",
        "        # Update weights\n",
        "        sgd_momentum(net.getParameters(), \n",
        "                     net.getGradParameters(), \n",
        "                     optimizer_config,\n",
        "                     optimizer_state)      \n",
        "        \n",
        "        train_loss += loss\n",
        "        history.append(loss)\n",
        "    train_loss /= n_iters\n",
        "\n",
        "\n",
        "    net.evaluate()\n",
        "    val_loss = 0\n",
        "    for x_batch, y_batch in get_batches((X_val, y_val), batch_size):\n",
        "        y_batch = y_batch.reshape(-1)\n",
        "        \n",
        "        # Forward\n",
        "        predictions = net.forward(x_batch)\n",
        "        val_loss += criterion.forward(predictions, y_batch)  \n",
        "\n",
        "    val_loss /= n_val_iters\n",
        "    val_losses.append(val_loss)\n",
        "        \n",
        "\n",
        "    # Visualize    \n",
        "    display.clear_output(wait=True)\n",
        "    #plt.figure(figsize=(8, 6))\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "    ax1.set_title(\"Training loss\")\n",
        "    ax1.set(xlabel=\"#Iteration\", ylabel=\"loss\")\n",
        "    ax1.plot(history, 'b')\n",
        "\n",
        "    ax2.set_title(\"Val loss\")\n",
        "    ax2.set(xlabel=\"#Epoch\", ylabel=\"loss\")\n",
        "    ax2.plot(val_losses, 'r')\n",
        "\n",
        "    plt.show()\n",
        "    print('[{}] train loss: {}\\tval loss: {}'.format(i, train_loss, val_loss))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAGDCAYAAAASzPzoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeZhcVb3u8e8vIwTCmEYgCYRZwkER\nAziBAxoSZNQwyRSmdPRw1euI1yPHg557cdajHEOYZ0SZMQgiiqCAROaIYBgTxiZIIIQhw7p/rKrT\n1ZXuTqfT1buq6/t5nnqqateurrfzJKl6a629dqSUkCRJkiSp3g0qOoAkSZIkST1hgZUkSZIkNQQL\nrCRJkiSpIVhgJUmSJEkNwQIrSZIkSWoIFlhJkiRJUkOwwEp1JiIGR8SiiNisL/ftRY5vR8S5ff1z\nJUkayCJiXESkiBjSxeNPRMRH+zuXNFBYYKXVVCqQ5cvyiHi94v7hq/rzUkrLUkprp5Se6st9JUnS\nykXEbyLilE627x8Rz3VVTCX1DwustJpKBXLtlNLawFPAvhXbLqre3zc+SZLq2nnAERERVduPBC5K\nKS0tIJOkEgusVGOlqbi/iIhLIuJV8pvieyPijoh4OSKejYj/ioihpf2HlKYejSvdv7D0+PUR8WpE\n3B4RW6zqvqXHJ0fEIxGxMCJ+GhF/ioipPfw9DoyIOaXMN0fEdhWP/Z+IeCYiXomIv0fEh0rb3xMR\nd5e2Px8R3+uDP1JJkmrpKmBDYPfyhohYH9gHOL90/+MRcU/p/W1eRHyzNy8UEcMj4sel99BnSreH\nlx4bFRHXld53X4qIWyNiUOmxr0bE06X3+ocjYs/V/J2lhmGBlfrHgcDFwLrAL4ClwOeAUcD7gUlA\nazfP/xTwDWAD8ijvt1Z134jYCLgM+HLpdR8Hdu1J+IjYHrgA+F9AC3ATcE1EDI2IHUrZd04prQNM\nLr0uwE+B75W2bw38qievJ0lSUVJKr5PfL4+q2Hww8PeU0n2l+6+VHl8P+Djw6Yg4oBcv93XgPcBO\nwDvJ78v/Vnrsi8B88vvu24D/A6TSF8gnAruklEYCewFP9OK1pYZkgZX6x20ppWtTSstTSq+nlO5K\nKd2ZUlqaUnoMmAl8sJvn/yqlNDultAS4iPxGt6r77gPcm1K6uvTYj4AXe5j/UOCalNLNpeeeSi7j\nu5HL+BrADhExJKX0eOl3AlgCbBMRG6aUXk0p3dnD15MkqUjnAVMiYo3S/aNK2wBIKf0hpfRA6X39\nfuASun8f78rhwCkppRdSSm3Af5CnKkN+D90E2DyltCSldGtKKQHLgOHA+IgYmlJ6IqX0aK9+S6kB\nWWCl/jGv8k5EvD0ifl1aDOIV4BTyqGhXnqu4vRhYuxf7blqZo/QmOL8H2cvPfbLiuctLzx2dUnqY\n/C3xKcALpanSG5d2PQYYDzwcEX+JiL17+HqSJBUmpXQb+UveAyJiK/LI6MXlxyNit4j4fUS0RcRC\nYDrdv493pcP7a+n2pqXb3wPmAjdGxGMRcVIp21zg88A3ye+7l0bEpkhNwgIr9Y9Udf904EFg69L0\n2pOB6sUi+tqzwJjyndLiFKN7+NxngM0rnjuo9LOeBkgpXZhSej+wBTAY+H+l7Q+nlA4FNgJ+AFxe\n8W22JEn17HzyyOsRwA0ppecrHrsYuAYYm1JaF5hB797HO7y/ApuVtlGaufTFlNKWwH7AF8rHuqaU\nLk4pfaD03AR8pxevLTUkC6xUjJHAQuC10vGl3R3/2leuA3aOiH1LKyF/jnxcTU9cBuwXER8qLTb1\nZeBV4M6I2D4iPlxadOL10mU5QEQcGRGjSiO2C8lvssv79teSJKkmzgc+CpxAxfThkpHASymlNyJi\nV/L6E71xCfBvEdESEaPIX2hfCBAR+0TE1qUvnBeSpw4vj4jtIuIjpffdN6h435WagQVWKsYXgaPJ\nJfB08sJONVX65vgQ4IfAAmAr4B7gzR48dw4578+BNvKiU/uVjocdDnyXPNXqOWB98qIUAHsDD5VW\nX/4+cEhK6a0+/LUkSaqJlNITwJ+BtcijrZU+A5xSen87mfxFb298G5gN3A88ANxd2gawDXnRxEXA\n7cB/p5R+T37fPZX2992NgK/18vWlhhP5MDhJzSYiBpOnKU1JKd1adB5JkiRpZRyBlZpIREyKiPVK\n046+QV7h8C8Fx5IkSZJ6xAIrNZcPAI+RpwHvBRyYUlrpFGJJkiSpHjiFWJIkSZLUEByBlSRJkiQ1\nBAusJEmSJKkhDCk6wKoaNWpUGjduXNExJEkDxF//+tcXU0o9PSeyOuF7sySpL3X33txwBXbcuHHM\nnj276BiSpAEiIp4sOkOj871ZktSXuntvdgqxJEmSJKkhWGAlSZIkSQ3BAitJkiRJaggWWEmSJElS\nQ7DASpIkSZIaggVWkiRJktQQLLCSJEmSpIZggZUkSZIkNQQLrCRJkiSpIVhgJUmSJEkNwQIrSZIk\nSWoITVlgU4LLL4cXXig6iSRJAmDZMrjuOliwoOgkkqQ61pQF9oknYMoUePvb4fnni04jSZJ4+GHY\nd18499yik0iS6lhTFtgxY+Daa+HVV+Eb3yg6jSRJYvx4eP/74fTT81QpSZI60ZQFduhQ2GcfOPFE\nOOssePzxohNJkiSmT4d//AN+//uik0iS6lRTFtiyz38eli+HSy4pOokkSWLKFNhgA5gxo+gkkqQ6\n1dQFdvPN4cMfhm9/Ox96I0mSCrTGGjB1Klx5pYtUSJI61dQFFvKhNq+/DjffXHQSSZLEtGmwdCmc\nfXbRSSRJdajpC+zWW8PIkfC3vxWdRJIksd128KEPwcyZ+TgfSZIqNH2BjcgLH86ZU3QSSZIE5MWc\nnngCbryx6CSSpDrT9AUWcoF1BFaSpDpx4IHQ0pKP85EkqYIFllxgn38eFiwoOokkSWLYMDj22HzS\n9qefLjqNJKmOWGDJBRbgoYeKzSFJkkpOOAGWLcsnbJckqcQCC+ywQ772OFhJkurEVlvBxIlwxhl5\nVWJJkrDAAjB2LAwZkteLkCRJdaK1FebPh+uvLzqJJKlOWGCBQYNgk03gmWeKTiJJkv7HvvvmN2gX\nc5IklVhgS0aPdp0ISZLqytChcNxxMGsWPPlk0WkkSXXAAltigZUkqQ6dcEI+afuZZxadRJJUByyw\nJZtu6hRiSZLqzmabweTJucAuWVJ0GklSwSywJZtuCq+8AosXF51EkiR1MH06PPdcPi+sJKmpWWBL\n1l8/X7/8crE5JElSlcmT8ykDZswoOokkqWAW2JJ11snXr7xSbA5JkvpCREyKiIcjYm5EnNTJ43tE\nxN0RsTQiplQ9tiwi7i1drum/1F0YPBiOPx5++1t49NGi00iSCmSBLbHASpIGiogYDJwGTAbGA4dF\nxPiq3Z4CpgIXd/IjXk8p7VS67FfTsD113HG5yM6cWXQSSVKBLLAlFlhJ0gCyKzA3pfRYSukt4FJg\n/8odUkpPpJTuB5YXEXCVjR6dzwt7zjnw5ptFp5EkFcQCW7Luuvl64cJic0iS1AdGA/Mq7s8vbeup\nNSJidkTcEREHdLZDREwr7TO7ra1tdbL23PTp0NYGV17ZP68nSao7FtgSR2AlSfofm6eUJgCfAn4c\nEVtV75BSmplSmpBSmtDS0tI/qT72MdhiCzj99P55PUlS3bHAllhgJUkDyNPA2Ir7Y0rbeiSl9HTp\n+jHgD8C7+jJcrw0aBNOmwR/+AH//e9FpJEkFsMCWjByZr51CLEkaAO4CtomILSJiGHAo0KPVhCNi\n/YgYXro9Cng/8LeaJV1VxxwDQ4a4mJMkNSkLbMnQoTBihCOwkqTGl1JaCpwI3AA8BFyWUpoTEadE\nxH4AEbFLRMwHDgJOj4g5padvD8yOiPuA3wOnppTqp8C+7W3wiU/AeefB668XnUaS1M+GFB2gnqyz\njiOwkqSBIaU0C5hVte3kitt3kacWVz/vz8CONQ+4Olpb4bLL4Fe/giOPLDqNJKkfOQJbYe21YdGi\nolNIkqRuffjDsO22LuYkSU3IAlthrbXgtdeKTiFJkroVkRdz+tOf4MEHi04jSepHFtgKFlhJkhrE\n0UfD8OGOwkpSk7HAVlhrLVi8uOgUkiRppUaNgilT4Pzz/fZZkpqIBbaCI7CSJDWQ1tZ8+oBf/KLo\nJJKkfmKBrWCBlSSpgXzgAzB+PMyYUXQSSVI/scBWsMBKktRAIvIo7F13wd13F51GktQPLLAVLLCS\nJDWYI4+ENdd0MSdJahIW2AojRuQCm1LRSSRJUo+svz4ccghcfDG8+mrRaSRJNVazAhsRYyPi9xHx\nt4iYExGf62SfiIj/ioi5EXF/ROxcqzw9sdZauby+8UaRKSRJ0iqZPh0WLYKLLio6iSSpxmo5ArsU\n+GJKaTzwHuBfI2J81T6TgW1Kl2nAz2uYZ6XWWitfO41YkqQGsuuu8M535mnETqOSpAGtZgU2pfRs\nSunu0u1XgYeA0VW77Q+cn7I7gPUiYpNaZVoZC6wkSQ0oIo/C3ntvXtBJkjRg9csxsBExDngXcGfV\nQ6OBeRX357NiySUipkXE7IiY3dbWVquYFlhJkhrVpz6V38g9pY4kDWg1L7ARsTZwOfD5lNIrvfkZ\nKaWZKaUJKaUJLS0tfRuwQrnALl5cs5eQJEm1sM46cPjhcOml8PLLRaeRJNVITQtsRAwll9eLUkpX\ndLLL08DYivtjStsKMXx4vn7zzaISSJKkXmtthddfhwsuKDqJJKlGarkKcQBnAQ+llH7YxW7XAEeV\nViN+D7AwpfRsrTKtzLBh+fqtt4pKIEmSem3nnWGXXVzMSZIGsFqOwL4fOBL4SETcW7rsHRHTI2J6\naZ9ZwGPAXOAM4DM1zLNS5RFYC6wkSQ2qtRXmzIE//anoJJKkGhhSqx+cUroNiJXsk4B/rVWGVVUe\ngXUKsSRJDerQQ+ELX8ijsB/4QNFpJEl9rF9WIW4UjsBKktTg1loLjjwSfvlLWLCg6DSSpD5mga3g\nMbCSJA0Ara15OtV55xWdRJLUxyywFZxCLEnSALDjjvC+97mYkyQNQBbYCk4hliRpgGhthUcegT/8\noegkkqQ+ZIGt4BRiSZIGiIMOgvXXhxkzik4iSepDFtgK5RFYpxBLktTg1lwTjj4arrwSXnih6DSS\npD5iga3gCKwkSQNIayssWQLnnFN0EklSH7HAVhg6NF87AitJ0gDw9rfDBz8IM2fC8uVFp5Ek9QEL\nbIWIPArrCKwkSQPE9Onw2GNw001FJ5Ek9QELbBULrCRJA8iBB8KoUS7mJEkDhAW2yvDhTiGWJGnA\nGD4cjj0WrrkGnnmm6DSSpNVkga3iCKwkSQPMCSfAsmVw1llFJ5EkrSYLbBULrCRJA8zWW8NHPwpn\nnJGLrCSpYVlgqziFWJI0EETEpIh4OCLmRsRJnTy+R0TcHRFLI2JKJ4+vExHzI+Jn/ZO4xqZPh3nz\n4Prri04iSVoNFtgqjsBKkhpdRAwGTgMmA+OBwyJifNVuTwFTgYu7+DHfAv5Yq4z9br/9YOON4fTT\ni04iSVoNFtgqw4dbYCVJDW9XYG5K6bGU0lvApcD+lTuklJ5IKd0PrHCC1Ih4N/A24Mb+CNsvhg6F\n446DWbPgqaeKTiNJ6iULbJVhw5xCLElqeKOBeRX355e2rVREDAJ+AHxpJftNi4jZETG7ra2t10H7\n1QknQEpw5plFJ5Ek9ZIFtopTiCVJTe4zwKyU0vzudkopzUwpTUgpTWhpaemnaKtp881h8uRcYJcs\nKTqNJKkXLLBVXMRJkjQAPA2Mrbg/prStJ94LnBgRTwDfB46KiFP7Nl6BWlvh2WfhuuuKTiJJ6gUL\nbJWhQ/1SVpLU8O4CtomILSJiGHAocE1PnphSOjyltFlKaRx5GvH5KaUVVjFuWHvvDWPGuJiTJDUo\nC2wVC6wkqdGllJYCJwI3AA8Bl6WU5kTEKRGxH0BE7BIR84GDgNMjYk5xifvRkCFw/PFwww3w2GNF\np5EkrSILbJUhQ2Dp0qJTSJK0elJKs1JK26aUtkop/Wdp28kppWtKt+9KKY1JKa2VUtowpbRDJz/j\n3JTSif2dveaOPx4GD4Yzzig6iSRpFVlgqzgCK0nSADd6NOyzD5x9tis3SlKDscBWscBKktQEpk+H\nF16Aq64qOokkaRVYYKtYYCVJagITJ8K4cTBjRtFJJEmrwAJbxWNgJUlqAoMGwQknwO9/Dw8/XHQa\nSVIPWWCrOAIrSVKTOPbY/M31zJlFJ5Ek9ZAFtooFVpKkJrHxxnDAAXDuufDGG0WnkST1gAW2igVW\nkqQmMn06vPQS/OpXRSeRJPWABbaKx8BKktREPvxh2HprOP30opNIknrAAltl6NBcYFMqOokkSaq5\nQYOgtRVuuw3mzCk6jSRpJSywVYYOzdeOwkqS1CSmToVhwxyFlaQGYIGtUi6wHgcrSVKTGDUKpkyB\n88+HxYuLTiNJ6oYFtsqQIfnaEVhJkppIayssXAi/+EXRSSRJ3bDAVnEEVpKkJrT77rD99k4jlqQ6\nZ4GtYoGVJKkJReRR2DvvhHvvLTqNJKkLFtgqFlhJkprUUUfBGms4CitJdcwCW6V8DKwFVpKkJrP+\n+nDIIXDhhfDqq0WnkSR1wgJbxdPoSJLUxKZPh0WL4JJLik4iSeqEBbaKU4glSWpiu+0G73gHzJgB\nKRWdRpJUxQJbxQIrSVITKy/mdM89MHt20WkkSVUssFU8BlaSpCZ3xBGw1lp5FFaSVFcssFU8BlaS\npCa3zjpw2GFw6aXw8stFp5EkVbDAVnEKsSRJYvp0WLw4r0gsSaobFtgqFlhJksS7350vp5/uYk6S\nVEcssFU8BlaSJAF5FPbBB+H224tOIkkqscBW8RhYSZIEwKGHwsiRLuYkSXXEAlvFKcSSJAmAtdeG\nI4+Eyy6Dl14qOo0kCQvsCiywkiTpf7S2wptvwnnnFZ1EkoQFdgUeAytJkv7HO94B732vizlJUp2w\nwFbxGFhJktRBays8/DDcckvRSSSp6VlgqziFWJIkdXDwwbDeenkUVpJUKAtsFacQS5KkDtZcE44+\nGi6/HF54oeg0ktTULLBVHIGVJA0EETEpIh6OiLkRcVInj+8REXdHxNKImFKxffPS9nsjYk5ETO/f\n5HWqtTV/ODj33KKTSFJTs8BW8RhYSVKji4jBwGnAZGA8cFhEjK/a7SlgKnBx1fZngfemlHYCdgNO\niohNa5u4AWy/PeyxR55GvHx50WkkqWnVrMBGxNkR8UJEPNjF4x+KiIWlb3jvjYiTa5VlVTgCK0ka\nAHYF5qaUHkspvQVcCuxfuUNK6YmU0v3A8qrtb6WU3izdHY5fdrdrbYXHHoPf/a7oJJLUtGr5pnQu\nMGkl+9yaUtqpdDmlhll6zGNgJUkDwGhgXsX9+aVtPRIRYyPi/tLP+E5K6ZlO9pkWEbMjYnZbW9tq\nB24In/wkjBoFM2YUnUSSmlbNCmxK6Y/AS7X6+bVigZUkNbuU0ryU0juArYGjI+JtnewzM6U0IaU0\noaWlpf9DFmH4cJg6Fa6+Gp59tug0ktSUip4W9N6IuC8iro+IHQrOAkBELrEeAytJamBPA2Mr7o8p\nbVslpZHXB4Hd+yhX45s2DZYtg7PPLjqJJDWlIgvs3cDmKaV3Aj8Frupqx/6epjR0qCOwkqSGdhew\nTURsERHDgEOBa3ryxIgYExFrlm6vD3wAeLhmSRvNNtvAnnvCzJm5yEqS+lVhBTal9EpKaVHp9ixg\naESM6mLffp2mNGSIBVaS1LhSSkuBE4EbgIeAy1JKcyLilIjYDyAidomI+cBBwOkRMaf09O2BOyPi\nPuAW4PsppQf6/7eoY9Onw1NPwQ03FJ1EkprOkKJeOCI2Bp5PKaWI2JVcphcUlaeSI7CSpEZX+nJ4\nVtW2kytu30WeWlz9vN8C76h5wEa2//7wtrflxZz23rvoNJLUVGpWYCPiEuBDwKjSN7z/DgwFSCnN\nAKYAn46IpcDrwKEppVSrPKvCAitJkro0dCgcdxyceirMmwdjx678OZKkPlHLVYgPSyltklIamlIa\nk1I6K6U0o1ReSSn9LKW0Q0rpnSml96SU/lyrLKtq6FAXcZIkSd044QRICc48s+gkktRUil6FuC55\nDKwkSerWuHEwaVIusH7rLUn9xgLbCacQS5KklWpthWeegeuuKzqJJDUNC2wnLLCSJGmlPv5xGD0a\nTj+96CSS1DQssJ3wGFhJkrRSQ4bA8cfn0+k8/njRaSSpKVhgO+ExsJIkqUeOPx4i4Iwzik4iSU3B\nAtsJpxBLkqQeGTMG9tkHzjoL3nqr6DSSNOBZYDthgZUkST3W2govvABXX110Ekka8CywnfAYWEmS\n1GN77QWbb+5iTpLUDyywnfAYWEmS1GODB8MJJ8Dvfgf/+EfRaSRpQLPAdsIpxJIkaZUce2z+Bnzm\nzKKTSNKAZoHthAVWkiStkk02gf33h3POgTfeKDqNJA1YFthOeAysJElaZdOnw4IFcMUVRSeRpAHL\nAtsJj4GVJEmr7CMfga22ghkzik4iSQOWBbYTTiGWJEmrbNCgfEqdW2+Fv/2t6DSSNCBZYDthgZUk\nSb0ydSoMG+YpdSSpRiywnfAYWEmS1CstLfCJT8D558PixUWnkaQBxwLbCY+BlSRJvTZ9Orz8Mlx2\nWdFJJGnAscB2winEkiSp1/bYA97+dqcRS1INWGA74RRiSZLUaxF5Mac77oD77is6jSQNKBbYTjiF\nWJIkrZajjoLhwx2FlaQ+ZoHtxNChsGwZpFR0EkmS1JA22AAOOQQuvBAWLSo6jSQNGBbYTgwZkq+d\nRixJknqttRVefRUuuaToJJI0YFhgO2GBlSRJq+2974Udd3QasST1IQtsJ4YOzdceBytJknqtvJjT\nX/8Ks2cXnUaSBgQLbCccgZUkSX3iiCNgxAhHYSWpj1hgO1EegbXASpKk1bLuunDYYXDxxbBwYdFp\nJKnhWWA7UR6BdQqxJElabdOnw+LFcNFFRSeRpIZnge2EI7CSJKnPTJgAO+8MM2Z4jj5JWk0W2E44\nAitJkvpUays88ADccUfRSSSpoVlgO+EIrCSp0UXEpIh4OCLmRsRJnTy+R0TcHRFLI2JKxfadIuL2\niJgTEfdHxCH9m3yAOuwwGDkyj8JKknrNAtsJR2AlSY0sIgYDpwGTgfHAYRExvmq3p4CpwMVV2xcD\nR6WUdgAmAT+OiPVqm7gJjBwJhx8Ol10GL71UdBpJalgW2E44AitJanC7AnNTSo+llN4CLgX2r9wh\npfRESul+YHnV9kdSSv8o3X4GeAFo6Z/YA9z06fDGG3D++UUnkaSG1aMCGxGfi4h1IjurNOVoYq3D\nFcURWElSvejle/BoYF7F/fmlbav62rsCw4BHO3lsWkTMjojZbW1tq/qjm9M73wm77ZbPCetiTpLU\nKz0dgT02pfQKMBFYHzgSOLVmqQrmCKwkqY4U8h4cEZsAFwDHpJSWVz+eUpqZUpqQUprQ0uIAbY9N\nnw5//zvcemvRSSSpIfW0wEbpem/ggpTSnIptA44jsJKkOtKb9+CngbEV98eUtvXsBSPWAX4NfD2l\n5LK5fengg2HddV3MSZJ6qacF9q8RcSP5zfOGiBhJ1TEzA0m5wDoCK0mqA715D74L2CYitoiIYcCh\nwDU9ebHS/lcC56eUfrUaudWZESPg6KPh8svBqdeStMp6WmCPA04CdkkpLQaGAsfULFXBnEIsSaoj\nq/wenFJaCpwI3AA8BFyWUpoTEadExH4AEbFLRMwHDgJOj4g5pacfDOwBTI2Ie0uXnWrymzWr1lZ4\n6y0499yik0hSwxnSw/3eC9ybUnotIo4AdgZ+UrtYxXIKsSSpjvTqPTilNAuYVbXt5Irbd5GnFlc/\n70LgwtUNrW6MHw+77w4zZ8IXvwiDPCmEJPVUT//H/DmwOCLeCXyRvBrhgF0D3hFYSVIdaar34KbR\n2gpz58LNNxedRJIaSk8L7NKUUiKfQ+5nKaXTgJG1i1UsR2AlSXWkqd6Dm8YnPwkbbphPqSNJ6rGe\nFthXI+Jr5KX7fx0Rg8jH4AxIjsBKkupIU70HN4011oCpU+Gqq+C554pOI0kNo6cF9hDgTfK56J4j\nHzPzvZqlKpgjsJKkOtJU78FNZdq0/G352WcXnUSSGkaPCmzpDfMiYN2I2Ad4I6U0YI+/cQRWklQv\nmu09uKlsuy185CN5Madly4pOI0kNoUcFNiIOBv5CXmr/YODOiJhSy2BFcgRWklQvmu09uOm0tsKT\nT8KNNxadRJIaQk9Po/N18vnnXgCIiBbgJmBAnuDcEVhJUh1pqvfgpnPAAbDRRjBjBkyeXHQaSap7\nPT0GdlD5jbNkwSo8t+E4AitJqiNN9R7cdIYNg2OPheuug/nzi04jSXWvp2+Av4mIGyJiakRMBX5N\n1cnRB5JygXUEVpJUB5rqPbgpnXACpARnnVV0Ekmqez1dxOnLwEzgHaXLzJTSV2sZrEjlKcSOwEqS\nitZs78FNacstYeJEOOMMvz2XpJXo6TGwpJQuBy6vYZa64QisJKmeNNN7cNOaPh0OPBBmzYL99is6\njSTVrW4LbES8CqTOHgJSSmmdmqQq2ODB+doCK0kqSrO+BzetffaBTTfNizlZYCWpS90W2JTSyP4K\nUk8i8iisU4glSUVp1vfgpjVkCBx/PHzrW/DEEzBuXNGJJKkuuYphF4YOdQRWkiT1o+OPz9+in3FG\n0UkkqW5ZYLvgCKwkSepXY8fCxz+eVyP2Q4gkdcoC2wVHYCVJUr9rbYXnn4erry46iSTVJQtsFxyB\nlSRJ/W7SJNhsMzj99KKTSFJdssB2wRFYSZLU7wYPhhNOgJtugrlzi04jSXWnZgU2Is6OiBci4sEu\nHo+I+K+ImBsR90fEzrXK0huOwEqSpEIce2wusjNnFp1EkupOLUdgzwUmdfP4ZGCb0mUa8PMaZlll\nQ4Y4AitJkgqw6aaw//5wzjnw5ptFp5GkulKzAptS+iPwUje77A+cn7I7gPUiYpNa5VlVQ4c6AitJ\nkgrS2govvghXXFF0EkmqK0UeAzsamFdxf35p2woiYlpEzI6I2W1tbf0SzhFYSZJUmI9+FLbc0sWc\nJKlKQyzilFKamVKakFKa0LrPqw8AACAASURBVNLS0i+v6SJOkiSpMIMGwbRpcMst8NBDRaeRpLpR\nZIF9GhhbcX9MaVtdcBEnSZJUqGOOyd+ou5iTJP2PIgvsNcBRpdWI3wMsTCk9W2CeDhyBlSRJhdpo\nI/jEJ+Dcc+H114tOI0l1oZan0bkEuB3YLiLmR8RxETE9IqaXdpkFPAbMBc4APlOrLL3hCKwkSSrc\n9Onw8svwy18WnUSS6sKQWv3glNJhK3k8Af9aq9dfXUOHwhtvFJ1CkiQ1tQ9+ELbbDmbMgKOOKjqN\nJBWuIRZxKoIjsJIkqXAR+ZQ6t98O999fdBpJKpwFtgseAytJkurC0UfD8OGeUkeSsMB2yRFYSZJU\nFzbYAA46CC64ABYtKjqNJBXKAtsFR2AlSVLdmD4dXn0VLr206CSSVCgLbBccgZUkNbKImBQRD0fE\n3Ig4qZPH94iIuyNiaURMqXrsNxHxckRc13+J1a33vQ922MFpxJKangW2C0OGOAIrSWpMETEYOA2Y\nDIwHDouI8VW7PQVMBS7u5Ed8Dziylhm1iiLyKOzs2fDXvxadRpIKY4HtwtChjsBKkhrWrsDclNJj\nKaW3gEuB/St3SCk9kVK6H1he/eSU0u+AV/slqXruiCNgzTUdhZXU1CywXXAEVpLUwEYD8yruzy9t\n6zMRMS0iZkfE7La2tr780erKeuvBYYfBxRfD888XnUaSCmGB7YKLOEmS1LWU0syU0oSU0oSWlpai\n4zSPL3wBli+HyZNh4cKi00hSv7PAdsFFnCRJDexpYGzF/TGlbWp0O+wAl18ODzwA++8Pb7xRdCJJ\n6lcW2C44AitJamB3AdtExBYRMQw4FLim4EzqK5Mnw7nnwi235CnFfmCR1EQssF1wBFaS1KhSSkuB\nE4EbgIeAy1JKcyLilIjYDyAidomI+cBBwOkRMaf8/Ii4FfglsGdEzI+Ivfr/t1C3Dj8cfvITuOqq\nvDpxSkUnkqR+MaToAPXKEVhJUiNLKc0CZlVtO7ni9l3kqcWdPXf32qZTn/jsZ6GtDb79bRg1Ck49\ntehEklRzFtguDBmSv8xctgwGDy46jSRJUidOOSWX2O98B1pa4ItfLDqRJNWUBbYLQ4fm66VLLbCS\nJKlORcBpp8GCBfClL+WR2KOPLjqVJNWMBbYLQ0p/MkuWwPDhxWaRJEnq0uDBcOGF8PLLcNxxsMEG\nsO++RaeSpJpwEaculAusx8FKkqS6N3w4XHEF7LwzHHww/PGPRSeSpJqwwHahPIXYlYglSVJDGDkS\nZs2CzTfPI7D33Vd0IknqcxbYLjgCK0mSGs6oUXDjjbDOOrDXXvDoo0UnkqQ+ZYHtQvm41zffLDaH\nJEnSKtlss1xilyyBiRPh2WeLTiRJfcYC24URI/L14sXF5pAkSVpl22+fpxM//zxMmpQXeJKkAcAC\n2wULrCRJami77ZYXdnroIdhvP3j99aITSdJqs8B2wQIrSZIa3sSJcMEFcNttcMghLu4hqeFZYLuw\n1lr52gIrSZIa2iGHwM9+BtdeC8cfD8uXF51IknptSNEB6pUjsJIkacD4zGegrQ2++c28UvH3vgcR\nRaeSpFVmge1CucC+9lqxOSRJkvrEySfDiy/CD34ALS3w1a8WnUiSVpkFtguOwEqSpAElAn7yk1xi\nTzopj8Qed1zRqSRplVhgu2CBlSRJA86gQXDeefDPf8K0abDBBnDggUWnkqQecxGnLlhgJUnSgDRs\nGFx+OeyyCxx2GPzhD0UnkqQes8B2YdgwGDLEAitJkgagtdaCX/8attwynyP27ruLTiRJPWKB7caI\nERZYSZI0QG24Idx4I6y/PkyaBP/4R9GJJGmlLLDdGDHCVYglSdIANmZMLrEpwcSJ8MwzRSeSpG5Z\nYLvhCKwkSRrwttsOrr8+r0681155gSdJqlMW2G44AitJkprChAlw1VXwyCOwzz5+gy+pbllgu7Hm\nmvDmm0WnkCRJ6gd77gkXXQS33w5TpsCSJUUnkqQVWGC7scYa8MYbRaeQJEnqJ1OmwIwZeUrxMcfA\n8uVFJ5KkDoYUHaCerbEGLFpUdApJkqR+NG1aPh7261+HUaPgRz+CiKJTSRJgge3W8OH5/29JkqSm\n8rWvQVsb/PjH0NKSy6wk1QELbDecQixJkppSBPzgB/mb/H/7tzwS29padCpJssB2xwIrSZKa1qBB\ncPbZ+bQ6n/40bLhhPkZWkgrkIk7dsMBKkqSmNnQoXHYZvO99cPjh8LvfFZ1IUpOzwHbDAitJkpre\niBFw7bWw3XZwwAEwe3bRiSQ1MQtsN9ZYw/PASpIksf768Jvf5GNhJ0+Gv/+96ESSmpQFthvlEdiU\nik4iSZJUsE03hd/+Nh8bO3EizJ9fdCJJTcgC243hw/P5u5cuLTqJJEmrJiImRcTDETE3Ik7q5PE9\nIuLuiFgaEVOqHjs6Iv5Ruhzdf6lV97beOo/ELlyYS+yCBUUnktRkLLDdWGONfO1xsJKkRhIRg4HT\ngMnAeOCwiBhftdtTwFTg4qrnbgD8O7AbsCvw7xGxfq0zq4G8611wzTXw2GOw996waFHRiSQ1EQts\nNyywkqQGtSswN6X0WErpLeBSYP/KHVJKT6SU7geWVz13L+C3KaWXUkr/BH4LTOqP0GogH/wgXHpp\nXtDpk5+Et94qOpGkJmGB7YYFVpLUoEYD8yruzy9t67PnRsS0iJgdEbPb2tp6HVQN7IAD4Iwz4MYb\n4aijYNmyohNJagJDig5QzyywkiR1LqU0E5gJMGHCBJc7bFbHHgsvvghf/SpsuCH87GcQUXQqSQOY\nBbYbFlhJUoN6GhhbcX9MaVtPn/uhquf+oU9SaWD6ylegrQ2+/31oaYFvfrPoRJIGMKcQd6NcYD0X\nrCSpwdwFbBMRW0TEMOBQ4JoePvcGYGJErF9avGliaZvUte9+F6ZOhf/4jzwKK0k14ghsN4YPz9eO\nwEqSGklKaWlEnEgunoOBs1NKcyLiFGB2SumaiNgFuBJYH9g3Iv4jpbRDSumliPgWuQQDnJJSeqmQ\nX0SNIyIfD/vSS/DZz+bpxIcdVnQqSQOQBbYb5RHYxYuLzSFJ0qpKKc0CZlVtO7ni9l3k6cGdPfds\n4OyaBtTAM2RIXpl40qS8qNMGG8BeexWdStIA4xTibrztbfn6ueeKzSFJktQQ1lwznyN2hx3gE5+A\nO+4oOpGkAcYC240xpe+l588vNockSVLDWHdd+M1vYJNN4OMfh7/9rehEkgYQC2w3RozIs1/mzVv5\nvpIkSSrZeON8fthhw2DiRHjyyaITSRogalpgI2JSRDwcEXMj4qROHp8aEW0RcW/pcnwt8/TG2LG5\nwM6b57GwkiRJPbbllnDDDbBoUS6xbW1FJ5I0ANSswEbEYOA0YDIwHjgsIsZ3susvUko7lS5n1ipP\nb5UL7LhxsPbasGxZ0YkkSZIaxDveAddeC089BXvvDa++WnQiSQ2uliOwuwJzU0qPpZTeAi4F9q/h\n69XERhvBggWwfDmkBNOnw047FZ1KkiSpQey+O/zyl3DPPXDAAfDmm0UnktTAallgRwOVR4/OL22r\n9smIuD8ifhURYzv7QRExLSJmR8Tstn6efrLeevmUZmVnngn33ZdLrSRJknpgn33g7LPh5pvh8MOd\n0iap14pexOlaYFxK6R3Ab4HzOtsppTQzpTQhpTShpaWlXwOutx68/vqK2//6136NIUmS1NiOOgp+\n8AO4/HL4zGfy1DZJWkW1LLBPA5UjqmNK2/5HSmlBSqk8j+RM4N01zNMr663X+fa77+7fHJIkSQ3v\nC1+Ak06CmTPhG98oOo2kBjSkhj/7LmCbiNiCXFwPBT5VuUNEbJJSerZ0dz/goRrm6ZWuCqznhpUk\nSeqF//t/4cUX4T//E1pa4HOfKzqRpAZSswKbUloaEScCNwCDgbNTSnMi4hRgdkrpGuCzEbEfsBR4\nCZhaqzy91VWBfe65/s0hSZI0IETAz3+eFxT5/Odhww3hiCOKTiWpQdRyBJaU0ixgVtW2kytufw34\nWi0zrK6uCuzzz/dvDkmSpAFjyBC4+OJ8ap1jjoENNsi3JWklil7Eqe6tu27n2x2BlSRJWg1rrAFX\nXZXPFTtlCvzpT0UnktQALLAr4RRiSZKkGllnHbj+ehgzJp9q54EHik4kqc5ZYFdi1Kj222uv3X57\n0SJ47bX+zyNJkjSgbLQR3HgjjBgBe+0Fjz9edCJJdcwCuxIjRrTfHj2642N33tm/WSRJkgakcePg\nhhvgjTdg4kQXG5HUJQvsKhhbOqvtOuvk6z33hCuvLC6PJEnSgPEv/wLXXQdPPw2TJ8PChUUnklSH\nLLA98MlP5uvNNsvX66/f/tg99/R/HkmSpAHpfe+Dyy/Px8Luv38ekZWkChbYHrj4YnjqqXyubei4\nsNO3vpUflyRJUh+YPBnOPRduuQUOOwyWLi06kaQ6YoHtgWHD8vTh8iJOw4Z1fPx73+v/TJIkSQPW\n4YfDT36ST7MzfTqkVHQiSXViSNEBGkl5QaflyztuHzy4/7NIkiQNaJ/9LLS1wbe/nU8LceqpRSeS\nVAcssKugXGCrvwS0wEqSJNXAKafkEvud7+Rjub74xaITSSqYBXYVdDUCO8iJ2JIkSX0vAk47DRYs\ngC99KY/EHn100akkFcjqtQrGjcvXO+zQcbsjsJIkSTUyeDBceCF89KNw3HFw7bVFJ5JUIAvsKthj\nD7jppnwoRiVHYCVJkmpo+HC44grYeWc4+GD44x+LTiSpIFavVbTnnjByZMdtixbBa6/B4sXFZJIk\nSRrwRo6EWbNg881h333hvvuKTiSpABbYXlhzzY73Fy6EjTeGt7+9mDySJElNYdQouPFGWGcd2Gsv\nePTRohNJ6mcW2F4YPrzj/X/+M4/CzptXTB5JkqSmsdlmucQuXQoTJ8KzzxadSFI/ssD2QuWiTd/4\nRi6wZS+/3HHf556DN97on1ySJElNYfvt83Ti55+HSZNW/AAmacCywK6mT386/x9aNmlSx8d32gl+\n+MP+zSRJkjTg7borXHklPPQQ7LcfvP560Ykk9QML7GraZBO47Ta46KJ8/847YdNN4a238qJOzz8P\nTz9dbEZJkqQB6WMfy6fYue02OOSQPK1Y0oBmge0DG2wAn/oU/PjH+f6zz+ZpxS++mO+7OrEkqb9F\nxKSIeDgi5kbESZ08PjwiflF6/M6IGFfaPiwizomIByLivoj4UD9Hl1bNwQfDaafl88MefzwsX150\nIkk1NKToAAPJmDHttxcvbj829rXXiskjSWpOETEYOA34GDAfuCsirkkp/a1it+OAf6aUto6IQ4Hv\nAIcAJwCklHaMiI2A6yNil5SSrUD169OfhrY2+Pd/zysVf+97EFF0Kkk14AhsL82ZA0891XFbZYG9\n7z645Zb22xF5doskSf1gV2BuSumxlNJbwKXA/lX77A+cV7r9K2DPiAhgPHAzQErpBeBlYEK/pJZW\nxze+ASeeCD/4AXz3u0WnkVQjjsD20vjxK24bPbr99oEHtt9+5JF8/bOfwQc+UNtckiQBo4HKk7vN\nB3brap+U0tKIWAhsCNwH7BcRlwBjgXeXrv9S+eSImAZMA9hss81q8CtIqygCfvITWLAATjoJBg2C\nT34Sxo3LtyUNCBbYPrTJJjB0KCxZ0vnjTz2VF8hbc83+zSVJ0io4G9gemA08CfwZWFa9U0ppJjAT\nYMKECak/A0pdGjQIzj03H8f1la/ky9prww47wL/8C+y4Y778y7/ARhsVnVZSL1hg+9DgwXD77TCh\ni4lWt98OI0bk42MtsZKkGnqaPGpaNqa0rbN95kfEEGBdYEFKKQH/u7xTRPwZeKS2caU+NGwYXHcd\n/OUv8MAD8OCD+fqqq+Css9r322ij9jJbvt5hh1x4JdUtC2wf68n/eU880fHcsZIk9bG7gG0iYgty\nUT0U+FTVPtcARwO3A1OAm1NKKSJGAJFSei0iPgYsrVr8Sap/gwfDe9+bL2Up5fMbVpbaBx6AmTM7\nnkN2yy1XHK3ddts8zU5S4SywfWyttVa+z6OP9r7APvooPPccvP/9vXu+JGngKx3TeiJwAzAYODul\nNCciTgFmp5SuAc4CLoiIucBL5JILsBFwQ0QsJ5ffI/v/N5BqIAI23jhfPvax9u3LlsHjj3cstQ8+\nCL/+dX4Mcnl9+9s7ltodd4TNNnO1Y6mfWWD7WE8LbHeWLs2n3ll33RUf23rrfJ082kiS1I2U0ixg\nVtW2kytuvwEc1MnzngC2q3U+qW4MHpw/YG29NRxwQPv2N96Av/+9vdg++CDceitcfHH7PiNHrjha\nu+OOsOGG/f97SE3CAtvHelJg58yBefNykf3Qhzo+tnx5nrmydCk880zXP2PZsvz/bU/cemv+f7Sz\nlZMlSZLUiTXWgJ12ypdKL7+cP8xVTkX+5S/zVOSyTTbpeGztjjvmD2IjRvTv7yANQBbYPjZs2Mr3\n+dWv8pd3r72WC2t55smSJfDQQ7ncrkxbW54B0xN77JGvHbWVJElaTeutl4/lqjyeKyV49tmOU5Af\neAD++7/zSC7kD3xbbdWx1O64Yx75HeJHcqmn/NdSgJdfbi+TgwbBSy/l05T97W9w6aXt+y1Z0nG9\ngOXL228/80zPCqylVZIkqcYiYNNN82Wvvdq3L1uWp9xVltoHH4Srr27/YDd8eF4cpbrYjh7t8bVS\nJyyw/WzcuLxC+69/3b7tjjvg97/Pt9va2rcvXAijRrXff+GF9tvPPtuz13vttV5HlSRJ0uoYPDiv\nYLzttnm0ouz11/O0u8pSe/PNcMEF7fust14utNXH2K6/fv//HlIdscDWwE9+kg+baG3NawFcdVX7\nYynB2LEd9//rX9tvVxbTl1/uWGCffrrz/brz/PM9zy1JkqR+sOaasPPO+VLppZfaj68tF9tLLoEZ\nM9r3GT16xdHa7bfPHz6lJmCBrYHPfjZfH3EEzJ/fscBCPq6/0uzZ7befe6799sKF7bcfeQSefLL9\n/m9/C8cf3/HnLF+epyRXssB29Nxz8P3vw777wgc/WHQaSZKkChtsALvvni9lKeUPlJWjtQ88kKfv\nvflm3mfQINhmmxVHa7faquerfkoNwgJbQyNG5NODVUppxQJ7113tt6tHYAFuvLHj4RQf/WheCOq1\n19pXPb7rLth1V7jllvZFm6DjtOP+9MorcMIJeTS6p4tN9YdzzoEf/AAee8wCK0mSGkBEnr43dixM\nnty+felSmDu3Y6m97z644or2RVDWXDOvfly9IvImm9TX8bXLl+ffp14vy5blP7d99oHttquvP7sm\nZIGtserZHCnBOuvk29tsA//4R8fT5VxzTfvthQvz/0WV5RVgzz3hppvgqafyjBHII7IAs2Z1LLBF\njcCefz5cdlmeAn3aacVk6Ex5hefK0W1JkqSGM2QIvP3t+XJQxSmdFy/OK4NWFtsbb4TzzmvfZ4MN\n2svs297W8yK3ZEltCmLRq44OGpT/PLu6pJQ/3H75y3lU++Mfz2V2jz3yIlzqVxbYfrDTTnk14bvu\nyn//11wzb99ss1xgIY9SPvdcPvSh7Kyz4MQTV/x573hHvv7zn/PPGjeu/d999RdC//xn++2U+u8L\no/JU5iVL2re98Qa8+iq0tPRPhs6UjyN+8cXiMkiSJNXMiBEwYUK+VHrxxVxoK6cin39+/nAG+cNq\ndyVuZZc11li959fi0pPfafDgFY/B68y8eXkV1uuuy+f8/a//grXXhokTc5nde+/8ZYBqzgLbD+65\nJxenMWPyF16TJsH/+l/whS/AFlvkfXbaCX7zm47PmzULHn6447aIPBME2o+BTal9Jfbqglqehgx5\nwbv+On/2smX5eunS9m3775+/ACzqS7Ybb2wf4bbASpKkpjJqFHzoQ/lSllK+9KTANbuxY2H69HxZ\nvDgfg3zddflyxRV5n113bR+dfde7nGpcI/5t7SejR8OPfgTXXgvDhuUvbcaNa3/8Xe/quP9WW+Xr\nRx/tuH3ddXMRrvbKK/l68eJclo87Lt+unCpbuUBUrZUL4ltvtW+78cZ8vWhR/+WoVDkV+5ln4Nhj\ni8khSZJUFyIsr70xYkQuqj//eT6m79574dvfzn+W3/wmvPvd+QP7tGl59MTzWvYp/8b2o89/fsVF\nncre//6O9+fOhR//eMX91l03z3aonKGwfDncf3++/eKL+d/N2WfnBZ6uvrp9v3Ip7q3XX4dvfStP\nBe5OSu3ns73oohVLeOXpgKC4f9PnnJP/v5k4ceW/kyRJkrSCCHjnO+HrX4fbb88L0Jx3Xv5w/4tf\n5CmIG26YpxifdlrH04qoVyywdaLyNGDl0+4cdlj7Ik1l5ZkIs2fnBeTWXhv++7/bRzcvugjOPLN9\n/87OF7twYfvhDmV3351XDAY4+ujOy+5Pfwonn5y/bKp0yy15dHPJkvZZKJX7/OxnHacSVxbY3/0u\n/w6//OWKr7eqLrgAvvGNPPW6WnnUd9tt8xcJZccdlxfAuuee1X99SZIkNbmWFjjqqLyaaVtb/rD7\nmc/k0akTT8xTMHfcEb72NfjTn9qPu1OPWWALds45eVpx5bGp+++frzfaKC8iV+mJJ/J1eVbCokX5\neNpVsd567SPBW26Zj7+dPh3+9//Oo6znn59PM/Pgg3nBqPJpfsrTgqvL70EH5QL95JOdT1NeY408\nk6Js/vz22/fdl6//9V87z5oS/OEPnf/bvv76nLmc56ij8uyNj398xeNsy3+O3/lOLrFl5VWJHYGV\nJElSnxo2DD7yEfjhD+GRR/LiNj/8Yf6Q//3vwwc+kG8fcQRcemnH1VfVJQtswaZOzcd5d7e4UvlY\n+2OOyQWsrHw6nrJtt82LrZWtvXb789daq+NrvPxyLnmPP55LZHmF5DvvbN/nL3/Ji9QdfHD+0uh7\n38vb583Lx/Nuu23+d1Y+h/att3YcST3wwDxj4skn26c4Qy6wS5fmRajK06Tb2uDvf8+jw1/7Wvu0\n4quvhg9/OJfVSnfckWdi/PjHeQS4urCWz3/76qv5586Zk+/vsEM+P+3Uqe2vW7m/JEmSVBPbbptH\nX373uzwydNlluQjccEOeetnSkj+4f//7+QNs0acXqlMW2DpRWTyrXXddPo707LPhK19p3z5yZPvt\n9dfPX+qURxL32itPqQVYsAA+97lcNI8+uv05lWW17MMfbr9d/hJo0aI8NbfszDPzCsr/+EcuueWF\nmo49Nr9OWUtLLtmXXNLxNW6+GQ49NJ8mqDwCCnm69Hrrwamn5seefTZ/SQW5QJel1PHP4aWXOq62\nDPD//l++njIl/9x77skjwVtumY8hrh617up8ua+9lgt1UQtPSZIkaQBad908jfG88/IUxttvh5NO\nyh9qv/zl/AF2m23yh+vf/rZ9xEgW2Hqz7rorbltrrVy8qlWOwJZPDzNoUC5jV1wB222Xtz3/fP4Z\ny5bl6cFl731v91m+9KV8/eKLeVZDZ558sut/T6NGrThKDPlLp8sv7/61H3sMNt00j+qWX6fsppvy\n9m9/O99fsCBPd670k5/kReHKxwZffXU+z/fgwfl+9UrOXY3AnnVWLtTf/373eevJT3+aT9fkl3aS\nJEkNYPBgeM978ofbe+/NH2J//vP84XXmzLzi6KhR8MlP5uMPuxp5aRIW2Dpyxx0rHvPanXI53HHH\nPIW+bKON8nTh8kJMBx+cC2xXDj64Z6+33npw2215GvOf/pS3XXRR10WpPAILeeGkH/84f5HUE+WF\nrCAX2d//Pv+7jcj/hkeOhC9+Md//whdgjz3yvuec0/68Rx7Jhx5A/n9g9907Zqt05535zx/yCPNX\nv5oXpSp78sk84vzPf+ZS+/rrK2ZOKZfz8jl5i/LZz+acrtguSZLUgMrnnL3uujxSc911+TjZv/wl\nT3nceGPYbTc45ZS8EmuTjVpYYOvIbrvlstZT5VK65pqdPz5sWC4yP/pRxwJbWeRgxVP4dGWHHfK+\nX/kKvO99+XX/+Mf8WEsL7Ltvnk5cPt9qS0v7v6cxY/IMiNGjV/46I0e2j7hef30eVb7vvvzvt2yz\nzfKU4PLPf9/74MIL8xTp8rmkDzig43loDz20/Xb1eaVvuimPSD/4YJ5e/N3vwpVX5unJAOeemw9b\n2GCDjsfuQp7x8c1v5p/x0Y/m55Y980zP/k95/PEVt116aZ49UlmkV0X5+F79//buPEqK6t4D+PfH\nrqDigkZFBZWIRAQVUdQniiSCG+5LfEnkEEyMJAHjHoNAPEieRnwaNxaV+ERFBOGgAgrEFYFBWRRU\nENmUTVlkE5iZ7/vjV2VVL7MvPd3z/ZxTp6tu3aq+t7pnqn51b90WERERyVL6zdkUCmCzWBjYFDcA\nVNOm/sxnPIANAzzAf4qntL8Pm/xTN/HWzpUr/W+mfv1oxOADDoieHQ0D8+TRhB96KOomHIoHh8ce\nm36U5fjv4ALezff66z0wvfRSPybxv9+ePYvuMt2+fTQ/eHD0HPGSJR6ApkN6C++6dR7cDhzo3Z4B\nfw4f8OduDz/cn10uzsSJ3kX8tdcS06+7zp/fD0eeLs6OHaktv+Go0blu+/bUZ6BFREREco5+cxaA\nAtisduqp3jU+PjJxUcIAtmFD74obWrXKR/fu0SNqOR00KHX7887zYDfummv8N1eHDPHW0FD4m68N\nGqQGsMmtif36JXZ/fuWVxIGmjjrKW3zrJH1Tk1ud40G4mR+X0ODBHkQmt7qG4i2m8QGnli/3Ftl2\n7VK3WbHCnzFu08aDd8C7OQP+sz9jx0YB6bx5fkyefz59a+rChYnbA4mttulaZ+PGj/fP9/HHE9Nr\nSwvs8cf7IGbJtmzRzyPlosWLgc6dge+/z3RJREREMqyW/uasAtgstu++fjHXsWPJecNW2jDw69rV\nX+vW9bRXX/Xfo501K7GrbSh50KNQ9+7+vGhcz57+evzxUStocgA7aVIU+MX16OFlWrfOA8FwdOZ4\n0A2ktjYmB9dhq2rftXxe9QAAF1lJREFUvtFgVMn69fPXrl19f0cckbh+xAi/uXXwwanbDhvmrxs3\nApMn+3zY8gr4oHLhyM177w0MH+6PLlx7rR+vVav854TI6Dnd+E9/xYPP889P/O3c8H2nT/efNrr8\nck8LR5UOA/V4C+wPP3gQPWGCt9Zm2oYN3tNl/Xq/2dC3r08bN/pn8d57pet6nZ8fjWSdnL9zZx/E\nryJ27/bRvSW9goLoBkx1mTzZH10oy3gBIiIiOa82/eYsyayaTjnlFErZffABCZCHHurLu3eTO3ak\nz7t6tecNpwceIDdvLtv7FRT46xVX+D6+/96X777bl1euTMx/yCGeXpS5c8k//IGcONHznX++p/fr\nR556amr+NWvIa68lv/229GW+5JKoznXrRvOzZiUej+KmevXINm0S05o0KTr/E0+Qt96aWKdly8gR\nIxLzXXUV+eST5JYt5PbtZKdOqfvq3Nm3r1fPlx980Jfz88l99iF/9rMo73PPJdZ9+3Zy2zY/zpdd\nRu7c6elLlvhx3LrVj+XLL5f+eCbbtYu8805y7Vpy4EAvR5cuiXV45hny/vt9/o03ErcPyxQ3Z060\n7aZNUfqWLZ7285+XvnxTp5IPP0wWFkZpffr4ftauTX3f0aNLt99XXiH79iXnzyeXL09ct25dNP/1\n1/66YQP5yCOJ5Qg99hi5eHHp3re0du0ily5NLNPTT5du2z/9yY/PihWVW6bi3HSTv+err1bePgHk\nsQac37J50rlZRKQG27yZHDOG/PWvyYMOii52O3f2C/3Fi9NfeGRQcefmjJ/0yjrpJFk+8+f7p92y\nZcl5CwvJAQN8mjOnYu+7bRs5b160nJ9PfvVVar61a72MJfn8c6/H0KEVK1c6mzZFActtt/nrXXf5\nujBIGjOGKYFjfDr4YPKII3z+6qvJdu2idQ0bpuZv2pQ86SSfb9CA/OKLaBkg9947dZujjkr/3kce\n6TcOzKI6FBYmBubxaeBAn/r08e9FfN2ECb7tqadGaSeeGO33nnv8uCxZ4sHL1Kk+T5Iffkg++qgH\nw59/7mmzZ0f76d49MZiOT/fcQx53nM/femv02WzdStap42V95BHyoYc8/amnom0XLIi+I/37e1rb\nttE+VqzwGxsFBb6/ZD/9qW9z1lnk+PHka69FZfngg8S8jRp5+qJF0b527iT37Endb3IdQ4MG+fKw\nYWTHjj4/e7bfyADIjz9O3M+GDdHnHxo9unR/N8UJv2/hDa1zz/XlMNi++Wa/cRQqLPTPmIyOw9Sp\nPn3zjd8cqyzffUdOm+Y3FjZt8s+va1d/z6eeqrz3UQCrc7OISK2Rn0/OnEn+9a+JF6rHHON3pqdO\nJX/4IdOlVAArfnEPeOCQ7VavrtqbRNu2+euSJVGrX/i3/d13TBt43X67vzZq5C2YYatd9+78sQX1\n7bcTt3n8cW8RD1tu0+135UoPZMKL9qKmsCXz6qsT08MWsrJO559f8nvOm5e4vN9+3oqYnO/NNz04\nT04P6x6fzjknmj/ppOgz+fDD1Ly7d5Onnx4tT5qU+FnFA+/49mFwe9llHqy2bu372X//1G3DADZs\nsS4o8Bsw8Tzt23v6YYf5MfjmG887eXL6cnfrRt5xh7eKJ68bOZLcay+f79jRey7s2OEt05MnR/me\nfdbPP/GgeNcucvBg8q23ouOWn++vK1d6UB5vaSX9HBXu46abPJg++GBfnj6dvPzyxPcgySFDfPmt\nt6J1fftG8xdc4MF5+/b+2X/0ETluHHnRRf6ZFRb6jYh7743+jidP9hvB8RZpMvHzDacWLaL5hQvL\n+MddBAWwOjeLiNRaK1d6l8ALL4zuTDdp4hcBTz+d2g2tmiiAlR8Dr+HDM12S7BReoBcU8MeL53ff\n9de8vMSgZvNm8p13fLuw1XLMGG+du+46JgQEixdHyw88wJSL9XhrVuvWqesBD0xGj46WzzzTuxvH\n88ydm37bkqZf/tKDopLynXZa2fc9dGjR6y680AOamTM9CA9bBePTCSf4a506/nrjjeQnn5SvnuF0\n992J3cfDacCAqNtzuunYY6P5Pn08mNxrr/RBanFT2J08nP74x9SbEuE0bVo0P3Ys2bx5dFxIbzVt\n2JD8z38St5s924PKZs3KVra//Y3s3TtaDrvylmVq3Jh8//1oecQID3h/8hNffvFFD9pnz/Y6lLS/\nKVMq5+9bAazOzSIiQn+mbNIk8ve/jy4sAL+rPnCgX1BWU1djBbAiFVRYGHUPzcuLWuTif8O9enmw\nFzdligchYb54ABwaOjRqPWzblgkX6HHhupEjvdtq+Gzt4sXe3TPcJiznhAm+HHYbjweH4fwPP0Tz\nF1+c+N633hq14N1xR+K6/v3JDh28pa2gIOreGk7puj6H08iR3iMgvAEQTl26kEcf7V2Ux48vevvk\nafRoD7TLGkylm8aM8S7Ia9Z4sBWmF9VCnjwdfbS/xlsuk6e//CWa//vfo2PfqlXUjbkiU6NG3osg\nLEvydMMNZP360XLYO6O4KeyWHp/C4LysQXpxU9Om0fwpp5Scv6jn+MtKAazOzSIikqSw0Lvb3Xef\nd4kKLwYOO8zvaE+YEHVbrAIZC2ABdAPwOYClAO5Ms74hgJeC9bMAtChpnzpJSra7/35vPUsnbIGb\nNClqhQpNm+bB6JYtvrx+ffScZGGhb9ewYZS/oMAHXdq40ZffeMOXw7yApw8e7P+DSG+Z27yZ/PTT\nxPfeuNG70w4fTv7731FgG3fkkb7Pu+/2VrRNm6LBwwB/nvrLL6P869Z5ehjsLloUrduwwVvlLrjA\ng1kznx8+nAkBTPjc7Y4dHhiXFPDEg8fWrX0goAce8K6o/fpFg42RUbfssEtvmzZ+rMJnVK+6ivzn\nPxO78s6cSZ5xRrTcuLEHpQMHetfcr7/24xKuD1vgb789sXX+oot8AKp42Xv39m63gHd9DtOLesYZ\n8GeYr7jCnxe98cbU9eEzymG36qlTyR49ovVDh/rnVNyz38uXk//6V2r6lVf6a16ef+4FBX6Mk/Md\neGA0f9RRHri2bBndNIi3fMdbjY8+Ov3fUHkogFUAKyIiJVi/nhw1yi+A9t2XP154du/uFwLJI1VW\nUEYCWAB1AXwJ4GgADQDMB9AmKc8fADwZzF8L4KWS9quTpOSyPXvI994r37aLFpGrVpUu7xVXeMtw\nZSosTO1VsnOnB2JFDboVDqqUPCp1so0bo31v3uxBzfr16fNt2eLPF+flkb/7HfnCC2TPnt4rJgya\n77uv5Pq88ooPzvfee941ORzPIOyOHb8JcfjhnhaOvj11Kvn88961d9w4T4sfm1tu8ZsUhYU+QNHS\npf6IScuWfi7YtcvzzZnj9Qn3u2ePj1K9Z493a+/dO3EgqxEjvLvzOed4627yKNzxwbOSP5MNG6L5\nIUP8uIX27PERkMNngidO9BsWgwZFeeLPLwNet/jI0KHRo72MF1/sXc/DUb5btUrMl5/vx3DrVn+2\nuEMHHxEb8M8x3YBZ5aUAVgGsiIiUwa5dfiHUr5+fwMOT/wknkDNmVMpbFHduNl9f+cysE4ABJM8P\nlu8Kfrbn/lieKUGemWZWD8BaAM1YTKE6dOjAvLy8KimziOS+jRuB/fbz3xsur/x8oF69aHndOmDZ\nMqBTp4qVbfVq/+m2Ll1Kv8327cAttwADBqT+HnKytWv9t1sPPRSoU45fAV+50n/D+aabot8bDm3a\nBCxYADRs6GexshyLRx8Fzj4baNcu/fpdu/z9Pv4YGDcOGDIk9f0rwszmkuxQeXusfXRuFhGpxb74\nAnjtNb9IePhhoG3bCu+yuHNzVQawVwLoRvK3wfKvAJxGsk8szydBntXB8pdBnm+L2q9OkiIiUpkU\nwFaczs0iIlKZijs3l+MefPUzsxvNLM/M8jZs2JDp4oiIiNR4ZtbNzD43s6Vmdmea9Q3N7KVg/Swz\naxGk1zezUWa20MwWhz2oREREaoKqDGC/BnBEbLl5kJY2T9CFeD8A3yXviOQwkh1IdmjWrFkVFVdE\nRCQ3mFldAI8B6A6gDYDrzKxNUrZeADaRPBbAUAD/CNKvAtCQZFsApwD4XRjcioiIZFpVBrBzALQy\ns5Zm1gA+SNPEpDwTAfwmmL8SwPTinn8VERGRUukIYCnJZSR3A3gRQI+kPD0AjArmxwI4z8wMAAE0\nDm4s7wVgN4Dvq6fYIiIixauyAJZkPoA+AKYAWAxgDMlPzWyQmV0SZBsJ4EAzWwrgFgApXZxERESk\nzA4HsCq2vDpIS5snOGdvAXAgPJjdDmANgJUAHiS5MfkN9HiPiIhkQr2Ss5QfydcBvJ6U1j82/wO8\nq5KIiIjUDB0BFAA4DMD+AN41s7dILotnIjkMwDDAB3Gq9lKKiEitlBWDOImIiEiZVGQcil8CmExy\nD8n1AN4HoFGaRUSkRlAAKyIiknsqMg7FSgBdAMDMGgM4HcBn1VJqERGREiiAFRERyTEVHIfiMQBN\nzOxTeCD8DMkF1VsDERGR9Kr0GVgRERHJjPKOQ0FyW7p0ERGRmkAtsCIiIiIiIpIVFMCKiIiIiIhI\nVlAAKyIiIiIiIllBAayIiIiIiIhkBfMR87OHmW0AsKKSdncQgG8raV81meqZW1TP3KJ6Zt5RJJtl\nuhDZTOfmaqdjVDo6TqWj41Q6Ok6lU1nHqchzc9YFsJXJzPJI5vyPs6ueuUX1zC2qp0gifVdKpmNU\nOjpOpaPjVDo6TqVTHcdJXYhFREREREQkKyiAFRERERERkaxQ2wPYYZkuQDVRPXOL6plbVE+RRPqu\nlEzHqHR0nEpHx6l0dJxKp8qPU61+BlZERERERESyR21vgRUREREREZEsUSsDWDPrZmafm9lSM7sz\n0+WpCDN72szWm9knsbQDzOxNM1sSvO4fpJuZPRLUe4GZnZy5kpeNmR1hZjPMbJGZfWpmfw7Sc6qu\nZtbIzGab2fygngOD9JZmNiuoz0tm1iBIbxgsLw3Wt8hk+cvKzOqa2cdmNilYztV6LjezhWY2z8zy\ngrSc+u4CgJk1NbOxZvaZmS02s065WE+pGrl0bq4q6c75kqqoawZJVNQ1h6RKvl6RVOmudapKrQtg\nzawugMcAdAfQBsB1ZtYms6WqkGcBdEtKuxPANJKtAEwLlgGvc6tguhHAE9VUxsqQD+AvJNsAOB3A\nzcHnlmt13QWgC8l2ANoD6GZmpwP4B4ChJI8FsAlAryB/LwCbgvShQb5s8mcAi2PLuVpPADiXZPvY\n0PK59t0FgP8FMJlkawDt4J9tLtZTKlkOnpuryrNIPedLqqKuGSRRUdcckir5ekXSS77WqRK1LoAF\n0BHAUpLLSO4G8CKAHhkuU7mRfAfAxqTkHgBGBfOjAFwaS/833YcAmprZodVT0oohuYbkR8H8Vvg/\nkcORY3UNyrstWKwfTATQBcDYID25nmH9xwI4z8ysmopbIWbWHMCFAEYEy4YcrGcxcuq7a2b7ATgb\nwEgAILmb5GbkWD2lyuTUubmqFHHOlyTFXDNITDHXHBKTfL0imVcbA9jDAayKLa9G7v1TO4TkmmB+\nLYBDgvmcqHvQffQkALOQg3UNuqnMA7AewJsAvgSwmWR+kCVelx/rGazfAuDA6i1xuT0M4HYAhcHy\ngcjNegJ+QTDVzOaa2Y1BWq59d1sC2ADgmaCb1Qgza4zcq6dUDX0fpEokXTNIkuRrDpI6TqmSr1ck\nvXTXOlWiNgawtQp9mOmcuZtmZk0AvAKgL8nv4+typa4kC0i2B9Ac3irROsNFqnRmdhGA9STnZros\n1eQskifDu0febGZnx1fmyHe3HoCTATxB8iQA2xF1FwaQM/UUkSxR3DWDuORrDjM7IdNlqklq4fVK\nRRR7rVOZamMA+zWAI2LLzYO0XLIu7IoXvK4P0rO67mZWH34iep7kuCA5J+sKAEH3yxkAOsG7V9YL\nVsXr8mM9g/X7AfiumotaHmcCuMTMlsO7CnaBPz+Za/UEAJD8OnhdD2A8/MZErn13VwNYHbt7PxYe\n0OZaPaVq6PsglaqIawYpQuyaQ89YJ0q5XjGz/8tskWqmIq51qkRtDGDnAGgVjHbaAMC1ACZmuEyV\nbSKA3wTzvwEwIZb+62D0z9MBbIl17avRgucdRwJYTPKh2KqcqquZNTOzpsH8XgB+Dn92ZwaAK4Ns\nyfUM638lgOnMgh93JnkXyeYkW8D/BqeTvB45Vk8AMLPGZrZPOA/gFwA+QY59d0muBbDKzI4Lks4D\nsAg5Vk+pMrXh3CzVpJhrBokp4prjs8yWqmYp4nrlvzNcrBqnmGudKlGv5Cy5hWS+mfUBMAVAXQBP\nk/w0w8UqNzN7AcA5AA4ys9UA7gUwBMAYM+sFYAWAq4PsrwO4AMBSADsA9Kz2ApffmQB+BWBh8KwG\nANyN3KvroQBGBSNy1gEwhuQkM1sE4EUzuw/AxwgGyglenzOzpfCBPa7NRKEr0R3IvXoeAmB8MOZU\nPQCjSU42sznIre8uAPwRwPNBALIMXvY6yL16SiXLtXNzVUl3zic5svitaqW01wwkX89gmWqitNcc\nGS6TZKe01zpV9WaWJY0YIiIiIiIiUsvVxi7EIiIiIiIikoUUwIqIiIiIiEhWUAArIiIiIiIiWUEB\nrIiIiIiIiGQFBbAiIiIiIiKSFRTAitQAZna/mZ1rZpea2V1B2rNmdmUw39fM9q7E97vUzNrElgeZ\nWdfK2r+IiEguKuZ8/ZWZzQumDyr5Pf9jZh0qc58i2UwBrEjNcBqADwF0BvBOmvV9AZQpgA1+160o\nlwL4MYAl2Z/kW2XZv4iISC1U1Pn6NpLtg+mMzBRNpHZQACuSQWb2gJktAHAqgJkAfgvgCTPrH8vz\nJwCHAZhhZjOCtF+Y2Uwz+8jMXjazJkH6cjP7h5l9BOAqM+ttZnPMbL6ZvWJme5vZGQAuAfBAcKf4\nmKTW3vPM7GMzW2hmT5tZw9i+BwbvudDMWlfjoRIREcmY0pyv02wzwMyeC87XS8ysd5Buwf4+Cc6n\n18S2uSNIm29mQ2K7u8rMZpvZF2b2X1VUTZGsoABWJINI3gagF4Bn4SfFBSRPJDkolucRAN8AOJfk\nuWZ2EIB7AHQleTKAPAC3xHb7HcmTSb4IYBzJU0m2A7AYQC+SHwCYiOhu8ZfhhmbWKCjLNSTbAqgH\n4KbYvr8N3vMJALdW6sEQERGpoUpxvg5vCs8zs+djm54IoAuATgD6m9lhAC4H0B5AOwBdg20PNbPu\nAHoAOC04b/9PbD/1SHaE98i6t8oqKpIF6mW6ACKCkwHMB9AaHmSW5HR499/3zQwAGsDvBodeis2f\nYGb3AWgKoAmAKSXs+zgAX5H8IlgeBeBmAA8Hy+OC17nwE7CIiEhtUdz5+jaSY9NsM4HkTgA7g15U\nHQGcBeAFkgUA1pnZ2/CguDOAZ0juAACSG2P7iZ9/W1RSfUSykgJYkQwxs/bwO7nNAXwLf8bVzGwe\n/E5tkZsCeJPkdUWs3x6bfxbApSTnm9kNAM6pWKmxK3gtgP5/iIhILVCB8zUAsITl0tL5VySgLsQi\nGUJyHsn2AL6At6hOB3B+0K13Z1L2rQD2CeY/BHCmmR0LAGbW2Mx+WsTb7ANgjZnVB3B9EfuL+xxA\ni3DfAH4F4O0yVk1ERCRnlPF8nayHmTUyswPhN5HnAHgXwDVmVtfMmgE4G8BsAG8C6Bn+6oCZHVA1\nNRLJbgpgRTIoOHFtIlkIoDXJRUVkHQZgspnNILkBwA0AXggGlJgJ786Uzt8AzALwPoDPYukvArgt\nGKzpmDCR5A8AegJ42cwWAigE8GS5KygiIpIDSnG+jj8DO8/MGgTpCwDMgN98/jvJbwCMD9Lnw4Ph\n20muJTkZPkZFXtC6q7EmRNIwsrw9GUREREREJB0zGwBgG8kHM10WkVyiFlgRERERERHJCmqBFRER\nERERkaygFlgRERERERHJCgpgRUREREREJCsogBUREREREZGsoABWREREREREsoICWBEREREREckK\nCmBFREREREQkK/w/E90ML1s4lYgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[5] train loss: 0.033980758290309\tval loss: 0.08135255659633887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnjc18_APUBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = np.argmax(net.forward(X_test), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCYNik8xQM0L",
        "colab_type": "code",
        "outputId": "1187205e-c015-46aa-db94-6e1b2397579c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "def accuracy(y_pred, y_true):\n",
        "    return sum(y_pred == y_true) / len(y_true)\n",
        "\n",
        "acc = accuracy(y_pred, y_test.reshape(-1))\n",
        "print(\"Accuracy on test: {}%\".format(acc*100))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test: 98.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-SUZu-YaCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h3MWQHEYZ-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0VPcz2zVyXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "cfd40e51-82bb-4b96-9770-afc91fb48cb9"
      },
      "source": [
        "print(\"Misclassified images\")\n",
        "misclass = (X_test[y_pred != y_test.reshape(-1)] + 1)/2*255\n",
        "misclass = misclass.astype(np.uint8)\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20,15))\n",
        "axes = axes.flatten()\n",
        "for img, ax in zip(misclass[:5].reshape((-1,28,28)), axes):\n",
        "    ax.imshow(img, cmap=\"Greys\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"True label:\\t\",*y_test[y_pred != y_test.reshape(-1)].reshape(-1)[:5])\n",
        "print(\"Classified as:\\t\",*y_pred[y_pred != y_test.reshape(-1)][:5])"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Misclassified images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAAEeCAYAAAAHC3ASAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8n8B5oUgnaSTjUwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x1080 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "True label:\t 9 1 7 2 4\n",
            "Classified as:\t 5 9 4 8 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}